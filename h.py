# -*- coding: utf-8 -*-
"""cysqtt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_e_giTqOMlR02kpmFc4JvYINcquCkR_Y
"""

#CYK
non_terminals = ["NP", "Nom", "Det", "AP", "Adv", "A"]
terminals = ["book", "orange", "man", "tall", "heavy", "very", "muscular"]

R = {
	"NP": [["Det", "Nom"]],
	"Nom": [["AP", "Nom"], ["book"],
			["orange"], ["man"]],
	"AP": [["Adv", "A"], ["heavy"],
			["orange"], ["tall"]],
	"Det": [["a"]],
	"Adv": [["very"], ["extremely"]],
	"A": [["heavy"], ["orange"], ["tall"],
		["muscular"]]
	}

# Function to perform the CYK Algorithm
def cykParse(w):
	n = len(w)
	T = [[set([]) for j in range(n)] for i in range(n)]
	for j in range(0, n):
		for lhs, rule in R.items():
			for rhs in rule:

				if len(rhs) == 1 and rhs[0] == w[j]:
					T[j][j].add(lhs)

		for i in range(j+1, -1, -1):

			for k in range(i, j ):
				for lhs, rule in R.items():
					for rhs in rule:

						if len(rhs) == 2 and rhs[0] in T[i][k] and rhs[1] in T[k + 1][j]:
							T[i][j].add(lhs)

	if len(T[0][n-1]) != 0:
		print(T)
	else:
		print("False",T)

w = "a heavy book".split()
cykParse(w)

#Probabilistic CYK
import nltk
from nltk import Tree, Nonterminal
from collections import defaultdict

# Sample PCFG training trees
trees = [
    "(S (NP John) (VP (VP (V called) (NP Mary)) (PP (P from) (NP Denver))))",
    "(S (NP John) (VP (V called) (NP Mary) (PP (P from) (NP Denver))))"
]

# Build rule probabilities
def extract_probabilistic_rules(tree_strings):
    rule_counts = defaultdict(int)
    lhs_counts = defaultdict(int)

    for s in tree_strings:
        for rule in Tree.fromstring(s).productions():
            rule_counts[rule] += 1
            lhs_counts[rule.lhs()] += 1

    return {rule: rule_counts[rule] / lhs_counts[rule.lhs()] for rule in rule_counts}

# Probabilistic CYK parser (structured like symbolic CYK)
def probabilistic_cyk(tokens, R):
    n = len(tokens)
    T = [[defaultdict(float) for _ in range(n)] for _ in range(n)]
    B = [[defaultdict(tuple) for _ in range(n)] for _ in range(n)]

    # Build reverse map: RHS -> list of (LHS, prob)
    rhs_to_lhs = defaultdict(list)
    for rule, prob in R.items():
        rhs_to_lhs[rule.rhs()].append((rule.lhs(), prob))

    # Fill diagonal
    for j in range(n):
        word = tokens[j]
        for lhs, prob in rhs_to_lhs.get((word,), []):
            T[j][j][lhs] = prob
            B[j][j][lhs] = word

    # CYK table filling
    for span in range(2, n+1):
        for i in range(n - span + 1):
            j = i + span - 1
            for k in range(i, j):
                for B_sym, B_prob in T[i][k].items():
                    for C_sym, C_prob in T[k+1][j].items():
                        for A_sym, rule_prob in rhs_to_lhs.get((B_sym, C_sym), []):
                            total_prob = rule_prob * B_prob * C_prob
                            if total_prob > T[i][j][A_sym]:
                                T[i][j][A_sym] = total_prob
                                B[i][j][A_sym] = (k, B_sym, C_sym)

    return T, B

# Tree reconstruction
def build_tree(B, i, j, sym):
    val = B[i][j].get(sym)
    if val is None:
        return None
    if isinstance(val, str):
        return Tree(sym.symbol(), [val])
    k, B_sym, C_sym = val
    left = build_tree(B, i, k, B_sym)
    right = build_tree(B, k+1, j, C_sym)
    return Tree(sym.symbol(), [left, right])

# === RUN ===
R = extract_probabilistic_rules(trees)
tokens = "John called Mary from Denver".split()
T, B = probabilistic_cyk(tokens, R)

root = Nonterminal("S")
if root in T[0][-1]:
    print(f"\nSentence is grammatically valid. Probability: {T[0][-1][root]:.6f}")
    tree = build_tree(B, 0, len(tokens)-1, root)
    print("\nMost Probable Parse Tree:")
    tree.pretty_print()
else:
    print("Sentence is not in the language.")

#seq2seq
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import random
import numpy as np
from torch.utils.data import Dataset, DataLoader

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 1. Load CSV
df = pd.read_csv('data.csv')  # CSV with 'source' and 'target' columns

# 2. Tokenization
def tokenize(sentence):
    return sentence.lower().strip().split()

# 3. Build Vocabulary
def build_vocab(sentences):
    vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2}
    for sentence in sentences:
        for word in tokenize(sentence):
            if word not in vocab:
                vocab[word] = len(vocab)
    return vocab

SRC_vocab = build_vocab(df['source'])
TRG_vocab = build_vocab(df['target'])
SRC_itos = {i: s for s, i in SRC_vocab.items()}
TRG_itos = {i: s for s, i in TRG_vocab.items()}

# 4. Encode sentence
def sentence_to_tensor(vocab, sentence):
    tokens = [vocab['<sos>']] + [vocab[word] for word in tokenize(sentence)] + [vocab['<eos>']]
    return torch.tensor(tokens, dtype=torch.long)

# 5. Dataset
class TranslationDataset(Dataset):
    def __init__(self, df, src_vocab, trg_vocab):
        self.data = df
        self.src_vocab = src_vocab
        self.trg_vocab = trg_vocab

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        src = sentence_to_tensor(self.src_vocab, self.data.iloc[idx]['source'])
        trg = sentence_to_tensor(self.trg_vocab, self.data.iloc[idx]['target'])
        return src, trg

def collate_fn(batch):
    src_batch, trg_batch = zip(*batch)
    src_lens = [len(x) for x in src_batch]
    trg_lens = [len(x) for x in trg_batch]
    src_padded = nn.utils.rnn.pad_sequence(src_batch, padding_value=SRC_vocab['<pad>'])
    trg_padded = nn.utils.rnn.pad_sequence(trg_batch, padding_value=TRG_vocab['<pad>'])
    return src_padded.to(device), trg_padded.to(device)

dataset = TranslationDataset(df, SRC_vocab, TRG_vocab)
dataloader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn, shuffle=True)

# 6. Model Config
INPUT_DIM = len(SRC_vocab)
OUTPUT_DIM = len(TRG_vocab)
HID_DIM = 256
EMB_DIM = 128
N_EPOCHS = 50
LEARNING_RATE = 0.01

# 7. Model Classes
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim)

    def forward(self, src):
        embedded = self.embedding(src)
        outputs, (hidden, cell) = self.lstm(embedded)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hid_dim)
        self.fc_out = nn.Linear(hid_dim, output_dim)

    def forward(self, input, hidden, cell):
        input = input.unsqueeze(0)
        embedded = self.embedding(input)
        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.embedding.num_embeddings
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)
        hidden, cell = self.encoder(src)
        input = trg[0, :]  # <sos>
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[t] = output
            top1 = output.argmax(1)
            input = trg[t] if random.random() < teacher_forcing_ratio else top1
        return outputs

# 8. Initialize
encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)
decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)
model = Seq2Seq(encoder, decoder).to(device)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
criterion = nn.CrossEntropyLoss(ignore_index=TRG_vocab['<pad>'])

# 9. Training Loop
for epoch in range(N_EPOCHS):
    model.train()
    epoch_loss = 0
    for src, trg in dataloader:
        optimizer.zero_grad()
        output = model(src, trg)
        output_dim = output.shape[-1]
        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)
        loss = criterion(output, trg)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {epoch_loss:.4f}")

# 10. Inference
def translate_sentence(model, sentence, src_vocab, trg_vocab, trg_itos, max_len=10):
    model.eval()
    src_tensor = sentence_to_tensor(src_vocab, sentence).unsqueeze(1).to(device)
    hidden, cell = model.encoder(src_tensor)
    input = torch.tensor([trg_vocab['<sos>']], device=device)
    translated_tokens = []
    for _ in range(max_len):
        output, hidden, cell = model.decoder(input, hidden, cell)
        top1 = output.argmax(1).item()
        if top1 == trg_vocab['<eos>']:
            break
        translated_tokens.append(trg_itos[top1])
        input = torch.tensor([top1], device=device)
    return translated_tokens

# 11. Try translating something from your vocab
print("Translation:", translate_sentence(model, "jai jaggu", SRC_vocab, TRG_vocab, TRG_itos))

!pip install datasets

### Transformer - MT

"""
DAta:

source	target

Hello, how are you?	Bonjour, comment ça va ?

What is your name?	Quel est ton nom ?

I am learning machine translation.	J'apprends la traduction automatique.

I love programming.	J'aime la programmation.

How old are you?	Quel âge as-tu ?

"""

import pandas as pd
from datasets import Dataset
from transformers import (
    MarianTokenizer,
    MarianMTModel,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq
)

# Config
model_name = "Helsinki-NLP/opus-mt-en-fr"
batch_size = 4
epochs = 3

# Load dataset (CSV with 'src' and 'tgt' columns)
df = pd.read_csv("data.csv")  # Your dataset here
print(df)
dataset = Dataset.from_pandas(df)

# Load model and tokenizer
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)

# Preprocessing
def preprocess(example):
    model_inputs = tokenizer(example["source"], max_length=128, truncation=True, padding="max_length")
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["target"], max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./finetuned-en-fr",
    evaluation_strategy="no",
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    num_train_epochs=epochs,
    weight_decay=0.01,
    save_total_limit=1,
    logging_dir='./logs',
    report_to="none"  # Disable WandB, Comet, etc.
)

# Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Fine-tune the model
trainer.train()

# Save model
model.save_pretrained("finetuned-marian-en-fr")
tokenizer.save_pretrained("finetuned-marian-en-fr")

from transformers import MarianMTModel, MarianTokenizer

def translate(text, model_dir="finetuned-marian-en-fr"):
    tokenizer = MarianTokenizer.from_pretrained(model_dir)
    model = MarianMTModel.from_pretrained(model_dir)
    encoded = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    generated = model.generate(**encoded)
    return tokenizer.decode(generated[0], skip_special_tokens=True)

# Example
print(translate("I am jaisu"))

# transformer - text summarization

import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    DataCollatorForSeq2Seq
)

# Config
model_name = "t5-small"  # T5 is good for summarization tasks
batch_size = 4
epochs = 3

# Sample data - Replace with your own summarization dataset
df =pd.read_csv('data.csv')
dataset = Dataset.from_pandas(df)

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Preprocessing
def preprocess(example):
    # For T5, we prefix the input with "summarize: "
    inputs = ["summarize: " + doc for doc in example["source"]]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")

    # Tokenize targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example["target"], max_length=128, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_dataset = dataset.map(preprocess, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./finetuned-summarizer",
    evaluation_strategy="no",
    learning_rate=5e-5,
    per_device_train_batch_size=batch_size,
    num_train_epochs=epochs,
    weight_decay=0.01,
    save_total_limit=1,
    logging_dir='./logs',
    report_to="none"  # Disable WandB, Comet, etc.
)

# Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Fine-tune the model
trainer.train()

# Save model
model.save_pretrained("finetuned-summarizer")
tokenizer.save_pretrained("finetuned-summarizer")

# Function to use the fine-tuned model for summarization
def summarize(text, model_dir="finetuned-summarizer"):
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)

    # Add the prefix required by T5
    inputs = tokenizer("summarize: " + text, return_tensors="pt", max_length=512, truncation=True)
    summary_ids = model.generate(inputs["input_ids"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

# Example
test_text = """
The development of artificial intelligence has accelerated dramatically in the past decade.
Machine learning models can now perform tasks that were once thought to require human intelligence.
This progress has been driven by advances in deep learning, increased computational power, and
the availability of large datasets. Applications of AI now span across healthcare, finance,
transportation, and entertainment industries. However, concerns about ethics, privacy, and
job displacement have also grown alongside these technological advancements.
"""

print(summarize(test_text))

# transformer - sentimental analysis

import pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# Config
model_name = "distilbert-base-uncased"  # Good base model for classification tasks
batch_size = 8
epochs = 3

# Sample data - Replace with your own sentiment dataset
data = {
    "text": [
        "I absolutely loved this product! It exceeded all my expectations.",
        "The service was terrible and the staff was rude.",
        "The movie was okay, not great but not awful either.",
        "This restaurant has the best pizza I've ever tasted!",
        "I was disappointed with the quality of the product.",
        "The hotel room was clean but rather small.",
        "The concert was amazing and the band played all my favorite songs!",
        "The customer support was unhelpful and took forever to respond.",
        "The book was interesting in parts but overall a bit boring.",
        "I highly recommend this app, it has made my life so much easier!"
    ],
    "label": [
        2,  # Positive
        0,  # Negative
        1,  # Neutral
        2,  # Positive
        0,  # Negative
        1,  # Neutral
        2,  # Positive
        0,  # Negative
        1,  # Neutral
        2   # Positive
    ]
}

# Label mapping (for reference)
id2label = {0: "negative", 1: "neutral", 2: "positive"}
label2id = {"negative": 0, "neutral": 1, "positive": 2}

df = pd.DataFrame(data)
print(df)
dataset = Dataset.from_pandas(df)

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3,  # 3 sentiment classes
    id2label=id2label,
    label2id=label2id
)

# Preprocessing function
def preprocess_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

# Apply preprocessing
tokenized_dataset = dataset.map(preprocess_function, batched=True)

# Data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# Define metrics computation function
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')

    return {"accuracy": accuracy, "f1": f1}

# Training arguments
training_args = TrainingArguments(
    output_dir="./finetuned-sentiment",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    num_train_epochs=epochs,
    weight_decay=0.01,
    save_strategy="epoch",
    save_total_limit=1,
    logging_dir='./logs',
    report_to="none",  # Disable WandB, Comet, etc.
    evaluation_strategy="no"  # Change to "epoch" if you have validation data
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Save model
model.save_pretrained("finetuned-sentiment")
tokenizer.save_pretrained("finetuned-sentiment")

# Function to use the model for sentiment analysis
def analyze_sentiment(text, model_dir="finetuned-sentiment"):
    tokenizer = AutoTokenizer.from_pretrained(model_dir)
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)

    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    outputs = model(**inputs)

    # Get prediction
    predictions = outputs.logits.softmax(dim=1)
    prediction = predictions.argmax().item()

    # Map to sentiment label
    sentiment = id2label[prediction]
    confidence = predictions[0][prediction].item()

    return {
        "sentiment": sentiment,
        "confidence": f"{confidence:.4f}",
        "scores": {id2label[i]: f"{score:.4f}" for i, score in enumerate(predictions[0].tolist())}
    }

# Example usage
examples = [
    "This new phone is fantastic! The camera quality is outstanding.",
    "The flight was delayed and the airline didn't provide any information.",
    "The weather is cloudy today, might rain later."
]

for example in examples:
    result = analyze_sentiment(example)
    print(f"Text: {example}")
    print(f"Sentiment: {result['sentiment']} (Confidence: {result['confidence']})")
    print(f"All scores: {result['scores']}")
    print()

# To split into train/test if needed
'''
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

# Then use train_dataset and test_dataset in your training
'''